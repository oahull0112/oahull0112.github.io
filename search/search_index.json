{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"howto/","title":"How-To","text":"Does this text change? A Second Level Header <p>Now is the time for all good men to come to the aid of their country. This is just a regular paragraph.</p> <p>The quick brown fox jumped over the lazy dog's back.</p> Header 3 <p>This is a blockquote.</p> <p>This is the second paragraph in the blockquote.</p> This is an H2 in a blockquote"},{"location":"Documentation/Applications/applications/","title":"Applications","text":"Applications Header page A Second Level Header <p>This is just a regular paragraph.</p> <p>The quick brown fox jumped over the lazy dog's back.</p> Header 3 <p>This is a blockquote.</p> <p>This is the second paragraph in the blockquote.</p> This is an H2 in a blockquote"},{"location":"Documentation/Applications/applications_template/","title":"Template for an Application Page","text":"<p>Documentation:  link to documentation</p> <p>Write a brief description of the program here.</p>"},{"location":"Documentation/Applications/applications_template/#getting-started","title":"Getting Started","text":"<p>This section provides the minimum amount of information necessary to successfully run a basic job on an NREL Cluster. This information should be as complete and self-contained as possible.</p> <p>Instructions should be step-by-step and include copy-and-pastable commands where applicable.</p> <p>For example, describe how the user can load the program module  with <code>module avail</code> and <code>module load</code>:</p> <pre><code>module avail program\n   program/2.0.0    program/1.0.0\n</code></pre> <pre><code>module load program/2.0.0\n</code></pre> <p>Include a section on how to run the job, e.g. with job script examples or commands for an interactive session.</p>"},{"location":"Documentation/Applications/applications_template/#example-job-scripts","title":"Example Job Scripts","text":"Kestrel CPU <pre><code>#!/bin/bash\n\n# In a comment summarize the hardware requested, e.g. number of nodes, \n    # number of tasks per node, and number of threads per task\n\n#SBATCH --time=\n#SBATCH --nodes=\n#SBATCH --ntasks-per-node=\n#SBATCH --cpus-per-task=\n#SBATCH --partition=\n#SBATCH --account=\n\n# include a section of relevant export and module load commands, e.g.:\n\nmodule load gcc/8.4.0\n\nexport OMP_NUM_THREADS=\n\n# include a sample srun command or similar\nsrun program.x\n</code></pre> Vermillion <p>If the submit script for Vermillion differs from Kestrel, then include a Vermillion example script here. If the submit script does not differ, then remove this section (starting from the <code>??? example \"Vermillion\"</code> line)</p> Swift <p>If the submit script for Swift differs from Kestrel, then include a Swift  example script here. If the submit script does not differ, then remove this section (starting from the <code>??? example \"Swift\"</code> line)</p> Template <p>Here's a template of a collapsible example.</p> <pre><code>You can include blocked sections\n</code></pre> <p>And unblocked sections.</p> <p>Note</p> <p>You can use a note to draw attention to the information in this section</p> <p>Include instructions on how to submit the job script</p>"},{"location":"Documentation/Applications/applications_template/#supported-versions","title":"Supported Versions","text":"Kestrel Swift Vermillion 0.0.0 0.0.0 0.0.0"},{"location":"Documentation/Applications/applications_template/#advanced","title":"Advanced","text":"<p>Include advanced user information about the code here (see BerkeleyGW page for some examples)</p> <p>One common \"advanced case\" might be that users want to build their own version of the code.</p>"},{"location":"Documentation/Applications/applications_template/#building-from-source","title":"Building From Source","text":"<p>Here, give detailed and step-by-step instructions on how to build the code, if this step is necessary. Include detailed instructions for how to do it on each applicable HPC system. Be explicit in your instructions. Ideally a user reading one of the build sections can follow along step-by-step and have a functioning build by the end.</p> <p>If building from source is not something anyone would reasonably want to do, remove this section.</p> <p>Be sure to include where the user can download the source code</p> Simple Build Instructions on Kestrel <p>Include here, for example, a Kestrel-specific makefile (see berkeleygw example page). This template assumes that we build the code with only one toolchain, which may not be the case. If someone might reasonably want to build with multiple toolchains, use the \"Multiple toolchain instructions on Kestrel\" template instead.</p> <p><pre><code>Include relevant commands in blocks.\n</code></pre> or as in-line <code>blocks</code></p> <p>Be sure to state how to set-up the necessary environment, e.g.:</p> <pre><code>module load gcc/8.4.0\nmodule load openmpi/3.1.6/gcc-8.4.0\nmodule load hdf5/1.10.6/gcc-ompi\n</code></pre> <p>Give instructions on compile commands. E.g., to view the available make targets, type <code>make</code>. To compile all program executables, type:</p> <pre><code>make cleanall\nmake all\n</code></pre> Multiple Toolchain Instructions on Kestrel Building with Cray MPICH <p>For example, put Cray MPICH-specific instructions here</p> <p><pre><code>Include relevant commands in blocks.\n</code></pre> or as in-line <code>blocks</code></p> <p>Be sure to state how to set-up the necessary environment, e.g.:</p> <pre><code>module load gcc/8.4.0\nmodule load openmpi/3.1.6/gcc-8.4.0\nmodule load hdf5/1.10.6/gcc-ompi\n</code></pre> <p>Give instructions on compile commands. E.g., to view the available make targets, type <code>make</code>. To compile all program executables, type:</p> <pre><code>make cleanall\nmake all\n</code></pre> Building with intel toolchain <p>For example, put intel-MPI specific instructions here</p> Building on Vermillion <p>information on how to build on Vermillion</p> Building on Swift <p>information on how to build on Swift</p>"},{"location":"Documentation/Applications/applications_template/#troubleshooting","title":"Troubleshooting","text":"<p>Include known problems and workarounds here, if applicable</p>"},{"location":"Documentation/Applications/applications_template_build/","title":"Template for an Application Page","text":"<p>Documentation:  link to documentation</p> <p>Write a brief description of the program here. Include a link to the program's website homepage.</p>"},{"location":"Documentation/Applications/applications_template_build/#getting-started","title":"Getting Started","text":"<p>This section provides the minimum amount of information necessary to successfully run a basic job on an NREL Cluster. This information should be as complete and self-contained as possible.</p> <p>Instructions should be step-by-step and include copy-and-pastable commands where applicable.</p> <p>This template is for applications that are not supported as modules by NREL, so are only accessible to users by building from source.</p>"},{"location":"Documentation/Applications/applications_template_build/#building-from-source","title":"Building From Source","text":"<p>Here, give detailed and step-by-step instructions on how to build the code, if this step is necessary. </p> <p>Include where to download the source code</p> Building on Kestrel <p>Be explicit in your instructions. Ideally a user reading one of these sections can following along step-by-step and have a functioning build by the end.</p> <p><pre><code>Include relevant commands in blocks.\n</code></pre> or as in-line <code>blocks</code></p> <p>Be sure to state how to set-up the necessary environment, e.g.:</p> <pre><code>module load gcc/8.4.0\nmodule load openmpi/3.1.6/gcc-8.4.0\nmodule load hdf5/1.10.6/gcc-ompi\n</code></pre> <p>Note</p> <p>You can use this section to draw attention to important information.</p> <p>Give instructions on compile commands. E.g., to view the available make targets, type <code>make</code>. To compile all program executables, type:</p> <pre><code>make cleanall\nmake all\n</code></pre> Building on Vermillion <p>information on how to build on Vermillion</p> Building on Swift <p>information on how to build on Swift</p> <p>Note</p> <p>You can use this section to draw attention to important information.</p> <p>Include a section on how to run the job, e.g. with job script examples or commands for an interactive session.</p>"},{"location":"Documentation/Applications/applications_template_build/#example-job-scripts","title":"Example Job Scripts","text":"Kestrel CPU <pre><code>#!/bin/bash\n\n# In a comment summarize the hardware requested, e.g. number of nodes, \n    # number of tasks per node, and number of threads per task\n\n#SBATCH --time=\n#SBATCH --nodes=\n#SBATCH --ntasks-per-node=\n#SBATCH --cpus-per-task=\n#SBATCH --partition=\n#SBATCH --account=\n\n# include a section of relevant export and module load commands, e.g.:\n\nmodule load gcc/8.4.0\n\nexport OMP_NUM_THREADS=\n\n# include a sample srun command or similar\nsrun program.x\n</code></pre> Vermillion <p>If the submit script for Vermillion differs from Kestrel, then include a Vermillion example script here. If the submit script does not differ, then remove this section (starting from the <code>??? example \"Vermillion\"</code> line)</p> Swift <p>If the submit script for Swift differs from Kestrel, then include a Swift  example script here. If the submit script does not differ, then remove this section (starting from the <code>??? example \"Swift\"</code> line)</p> Template <p>Here's a template of a collapsible example.</p> <pre><code>You can include blocked sections\n</code></pre> <p>And unblocked sections.</p>"},{"location":"Documentation/Applications/applications_template_build/#advanced","title":"Advanced","text":"<p>Include advanced user information about the code here (see BerkeleyGW pages for example)</p>"},{"location":"Documentation/Applications/applications_template_build/#troubleshooting","title":"Troubleshooting","text":"<p>Include known problems and workarounds here, if applicable</p>"},{"location":"Documentation/Applications/applications_template_module/","title":"Template for an Application Page","text":"<p>Documentation  link to documentation</p> <p>Write a brief description of the program here.</p>"},{"location":"Documentation/Applications/applications_template_module/#getting-started","title":"Getting Started","text":"<p>This section provides the minimum amount of information necessary to successfully run a basic job on an NREL Cluster. This information should be as complete and self-contained as possible.</p> <p>Instructions should be step-by-step and include copy-and-pastable commands where applicable.</p> <p>For example, describe how the user can load the program module  with <code>module avail</code> and <code>module load</code>:</p> <pre><code>module avail program\n   program/2.0.0    program/1.0.0\n</code></pre> <pre><code>module load program/2.0.0\n</code></pre> <p>Include a section on how to run the job, e.g. with job script examples or commands for an interactive session.</p>"},{"location":"Documentation/Applications/applications_template_module/#example-job-scripts","title":"Example Job Scripts","text":"Kestrel CPU <pre><code>#!/bin/bash\n\n# In a comment summarize the hardware requested, e.g. number of nodes, \n    # number of tasks per node, and number of threads per task\n\n#SBATCH --time=\n#SBATCH --nodes=\n#SBATCH --ntasks-per-node=\n#SBATCH --cpus-per-task=\n#SBATCH --partition=\n#SBATCH --account=\n\n# include a section of relevant export and module load commands, e.g.:\n\nmodule load gcc/8.4.0\n\nexport OMP_NUM_THREADS=\n\n# include a sample srun command or similar\nsrun program.x\n</code></pre> Vermillion <p>If the submit script for Vermillion differs from Kestrel, then include a Vermillion example script here. If the submit script does not differ, then remove this section (starting from the <code>??? example \"Vermillion\"</code> line)</p> Swift <p>If the submit script for Swift differs from Kestrel, then include a Swift  example script here. If the submit script does not differ, then remove this section (starting from the <code>??? example \"Swift\"</code> line)</p> Template <p>Here's a template of a collapsible example.</p> <pre><code>You can include blocked sections\n</code></pre> <p>And unblocked sections.</p> <p>Note</p> <p>You can use a note to draw attention to the information in this section</p> <p>Note</p> <p>If the submit scripts for Vermillion, Swift, and Kestrel are all the same, remove all of the submit script collapsible sections (remove the <code>??? example</code> headers that are inside the <code>### Example Job Scripts</code> header), and give a submit script directly below the <code>### Example Job Scripts</code> header.</p> <p>Include instructions on how to submit the job script</p>"},{"location":"Documentation/Applications/applications_template_module/#advanced","title":"Advanced","text":"<p>Include advanced user information about the code here (see BerkeleyGW pages for example)</p>"},{"location":"Documentation/Applications/applications_template_module/#troubleshooting","title":"Troubleshooting","text":"<p>Include known problems and workarounds here, if applicable</p>"},{"location":"Documentation/Applications/berkeleygw_build/","title":"BerkeleyGW","text":"<p>Documentation: BerkeleyGW</p> <p>BerkeleyGW is a massively parallel many-body perturbation theory code capable of performing RPA, GW, and GW-BSE calculations, which can be used to investigate properties of materials with high accuracy.</p>"},{"location":"Documentation/Applications/berkeleygw_build/#getting-started","title":"Getting Started","text":"<p>This section provides the minimum amount of information needed to run a BerkeleyGW job on an NREL cluster.</p> <p>BerkeleyGW must be built from source to run on NREL systems.</p>"},{"location":"Documentation/Applications/berkeleygw_build/#building-instructions","title":"Building Instructions","text":"<p>First, download BerkeleyGW.</p> <p>Then, follow the build instructions in the \"building\" drop-downs below for the cluster you will be running on.</p> Building on Kestrel <p>The following <code>arch.mk</code> file was used to build BerkeleyGW-3.0 on Kestrel on (date).     Copy this arch.mk file into your BerkeleyGW directory.</p> <pre><code>COMPFLAG  = -DGNU\nPARAFLAG  = -DMPI -DOMP\nMATHFLAG  = -DUSESCALAPACK -DUNPACKED -DUSEFFTW3 -DHDF5\n\nFCPP    = /usr/bin/cpp -C\nF90free = mpifort -ffree-form -ffree-line-length-none -fopenmp -fno-second-underscore -cpp\nLINK    = mpifort -fopenmp\n# FHJ: -funsafe-math-optimizations breaks Haydock and doesn't give any significant speedup\nFOPTS   = -O3 -funroll-loops \nFNOOPTS = $(FOPTS)\nMOD_OPT = -J  \nINCFLAG = -I\n\nC_PARAFLAG  = -DPARA\nCC_COMP = mpiCC\nC_COMP  = mpicc\nC_LINK  = mpicc\nC_OPTS  = -O3 -ffast-math\nC_DEBUGFLAG = \n\nREMOVE  = /bin/rm -f\n\n# Math Libraries                                                                                                                                                                                            \nFFTWPATH     =  /projects/scatter/mylibraries_CentOS77/\n#/nopt/nrel/apps/fftw/3.3.3-impi-intel/\n#FFTWLIB      = $(FFTWPATH)/lib/libfftw3.a\nFFTWLIB      =  $(FFTWPATH)/lib/libfftw3_omp.a $(FFTWPATH)/lib/libfftw3.a\nFFTWINCLUDE  =  $(FFTWPATH)/include\n\nLAPACKLIB = /projects/scatter/mylibraries_CentOS77/lib/libopenblas.a\n\nSCALAPACKLIB = /projects/scatter/mylibraries_CentOS77/lib/libscalapack.a\n\nHDF5PATH      = /nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hdf5-1.10.6-dj4jq2ffttkdxksimqe47245ryklau4a\nHDF5LIB      =  ${HDF5PATH}/lib/libhdf5hl_fortran.a \\\n                ${HDF5PATH}/lib/libhdf5_hl.a \\\n                ${HDF5PATH}/lib/libhdf5_fortran.a \\\n                ${HDF5PATH}/lib/libhdf5.a /home/ohull/.conda-envs/bgw/lib/libsz.a -lz -ldl\nHDF5INCLUDE  = ${HDF5PATH}/include\n\nPERFORMANCE  =\n\nTESTSCRIPT = \n</code></pre> <p>Then, load the following modules:</p> <pre><code>module load gcc/8.4.0\nmodule load openmpi/3.1.6/gcc-8.4.0\nmodule load hdf5/1.10.6/gcc-ompi\n</code></pre> <p>Choose whether to use the real or complex flavor of BerkeleyGW by copying the corresponding file to flavor.mk. For example, for the complex version:</p> <p><code>cp flavor_cplx.mk flavor.mk</code></p> <p>Note</p> <p>If you are unsure which to use, select the complex flavor.</p> <p>Finally, compile the code. To view the available make targets, type <code>make</code>. To compile all BerkeleyGW executables, type: <pre><code>make cleanall\nmake all\n</code></pre></p> Building on Vermillion <p>information on how to build on Vermillion</p> Building on Swift <p>information on how to build on Swift</p> <p>Once the code is successfully built, we can submit a job. Be sure to include the proper <code>module load</code> commands in your submit script. These should match the modules you loaded to build the code.</p>"},{"location":"Documentation/Applications/berkeleygw_build/#sample-job-scripts","title":"Sample Job Scripts","text":"Kestrel CPU <pre><code>#!/bin/bash\n\n# This job script uses 72 MPI tasks across 2 nodes (36 tasks per node)\n\n#SBATCH --time=01:00:00\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=36\n#SBATCH --partition=standard\n#SBATCH --account=\n\nmodule load gcc/8.4.0\nmodule load openmpi/3.1.6/gcc-8.4.0\nmodule load hdf5/1.10.6/gcc-ompi\n\nBGW=/path/to/where/you/built/BerkeleyGW\n\nsrun $BGW/epsilon.cplx.x\n</code></pre> Kestrel GPU <p>Put job example here</p> Vermillion <p>Put job example here</p> Swift <p>Put job example here</p> <p>Save the submit file as bgw.in, and submit with the command</p> <p><code>sbatch bgw.in</code></p>"},{"location":"Documentation/Applications/berkeleygw_build/#advanced","title":"Advanced","text":""},{"location":"Documentation/Applications/berkeleygw_build/#io-settings","title":"IO Settings","text":"<p>For large systems, the wavefunction binary file format yields significantly slower read-in times relative to an HDF5-format wavefunction file. The BerkeleyGW code includes utilities to convert wavefunction binary files to HDF5 format and vice-versa called hdf2wfn.x and wfn2hdf.x (see documentation). It is recommended to use HDF5-formatted wavefunction files where possible.</p>"},{"location":"Documentation/Applications/berkeleygw_build/#lustre-file-striping","title":"Lustre File Striping","text":"<p>BerkeleyGW supports wavefunction files in HDF5 format and binary format. Wavefunction inputs to BerkeleyGW can become large depending on the system under investigation. Large (TODO: define large for Kestrel. Probably &gt; 10 GB) HDF5 wavefunction files benefit from Lustre file striping, and the BerkeleyGW code can see major runtime speed-ups when using this feature.</p> <p>Tip</p> <p>Binary format wavefunction files do not benefit from Lustre file striping</p> <p>For more on Lustre file striping, see (TODO: documentation section on Lustre file striping?)</p>"},{"location":"Documentation/Applications/berkeleygw_build/#advanced-submission-script-example","title":"Advanced submission script example","text":"<p>Because multiple executables in BerkeleyGW require the WFN input files (WFN and WFNq), we can streamline the file linking inside a submission script. We can also include the Lustre file striping step in our submission script. The below example script shows how this can be done for the BerkeleyGW epsilon executable.</p> Advanced submit script <pre><code>#!/bin/bash\n#SBATCH -t 00:20:00\n#SBATCH -N 8\n#SBATCH --gpus-per-node=4\n#SBATCH -C gpu\n#SBATCH -o BGW_EPSILON_%j.out\n#SBATCH --account=\n\nBGW=/path/to/where/you/built/BerkeleyGW/bin\nWFN_folder=/path/to/folder/that/contains/WFN/and/WFNq\n\nmkdir BGW_EPSILON_$SLURM_JOBID\nlfs setstripe -c 60 BGW_EPSILON_$SLURM_JOBID\ncd    BGW_EPSILON_$SLURM_JOBID\nln -s $BGW/epsilon.cplx.x .\nln -s  ../epsilon.inp .\nln -sf  ${WFN_folder}/WFNq.h5      .   \nln -sf  ${WFN_folder}/WFN.h5   ./WFN.h5\n\nulimit -s unlimited\nexport OMP_PROC_BIND=true\nexport OMP_PLACES=threads\nexport BGW_WFN_HDF5_INDEPENDENT=1\n\nexport OMP_NUM_THREADS=16\nsrun -n 32 -c 32 --cpu-bind=cores ./epsilon.cplx.x\n</code></pre>"},{"location":"Documentation/Applications/berkeleygw_build/#troubleshooting","title":"Troubleshooting","text":"<p>Include known problems and workarounds here, if applicable</p>"},{"location":"Documentation/Applications/berkeleygw_example/","title":"BerkeleyGW","text":"<p>Documentation: BerkeleyGW</p> <p>BerkeleyGW is a massively parallel many-body perturbation theory code capable of performing RPA, GW, and GW-BSE calculations, which can be used to investigate properties of materials with high accuracy.</p>"},{"location":"Documentation/Applications/berkeleygw_example/#getting-started","title":"Getting Started","text":"<p>This section provides the minimum amount of information needed to run a BerkeleyGW job on an NREL cluster.</p> <p>First, see which versions of BerkeleyGW are available with <code>module avail</code> and load your preferred version with <code>module load</code>:</p> <p><pre><code>module avail berkeleygw\n   berkeleygw/3.0.1-cpu    berkeleygw/3.0.1-gpu (D)\n</code></pre> The <code>module avail berkeleygw</code> command shows that two BerkeleyGW modules are available. To select the GPU-enabled version of BerkeleyGW, for example, we use the <code>module load</code> command:</p> <pre><code>module load berkeleygw/3.0.1-gpu\n</code></pre> <p>Next, create a job script. Below are example job scripts for the available NREL systems. Continuing the above example, we would select the \"Kestrel GPU\" example script.</p>"},{"location":"Documentation/Applications/berkeleygw_example/#sample-job-scripts","title":"Sample Job Scripts","text":"Kestrel CPU <pre><code>#!/bin/bash\n\n# This job requests 72 MPI tasks across 2 nodes (36 tasks/node) and no threading\n\n#SBATCH --time=01:00:00\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=36\n#SBATCH --partition=standard\n#SBATCH --account=\n\nmodule load berkeleygw/3.0.1-cpu\n\nsrun epsilon.cplx.x\n</code></pre> Kestrel GPU <p>Put job example here</p> Vermillion <p>Put job example here</p> Swift <p>Put job example here</p> <p>Save the submit file as bgw.in, and submit with the command</p> <p><code>sbatch bgw.in</code></p>"},{"location":"Documentation/Applications/berkeleygw_example/#supported-versions","title":"Supported Versions","text":"Kestrel Swift Vermillion 2.0, 3.0 0.0.0 0.0.0"},{"location":"Documentation/Applications/berkeleygw_example/#advanced","title":"Advanced","text":""},{"location":"Documentation/Applications/berkeleygw_example/#io-settings","title":"IO Settings","text":"<p>For large systems, the wavefunction binary file format yields significantly slower read-in times relative to an HDF5-format wavefunction file. The BerkeleyGW code includes utilities to convert wavefunction binary files to HDF5 format and vice-versa called hdf2wfn.x and wfn2hdf.x (see documentation). It is recommended to use HDF5-formatted wavefunction files where possible.</p>"},{"location":"Documentation/Applications/berkeleygw_example/#lustre-file-striping","title":"Lustre File Striping","text":"<p>BerkeleyGW supports wavefunction files in HDF5 format and binary format. Wavefunction inputs to BerkeleyGW can become large depending on the system under investigation. Large (TODO: define large for Kestrel. Probably &gt; 10 GB) HDF5 wavefunction files benefit from Lustre file striping, and the BerkeleyGW code can see major runtime speed-ups when using this feature.</p> <p>Tip</p> <p>Binary format wavefunction files do not benefit from Lustre file striping</p> <p>For more on Lustre file striping, see (TODO: documentation section on Lustre file striping?)</p>"},{"location":"Documentation/Applications/berkeleygw_example/#advanced-submission-script-example","title":"Advanced submission script example","text":"<p>Because multiple executables in BerkeleyGW require the WFN input files (WFN and WFNq), we can streamline the file linking inside a submission script. We can also include the Lustre file striping step in our submission script. The below example script shows how this can be done for the BerkeleyGW epsilon executable.</p> Advanced submit script <p>This script assumes you build your own version of BerkeleyGW. If not, remove the <code>BGW=/path/to/where/you/built/BerkeleyGW/bin</code> and <code>ln -s $BGW/epsilon.cplx.x .</code> lines.</p> <p>Be sure to load the proper modules (see Getting Started if not building your own version.)</p> <pre><code>#!/bin/bash\n#SBATCH -t 00:20:00\n#SBATCH -N 8\n#SBATCH --gpus-per-node=4\n#SBATCH -C gpu\n#SBATCH -o BGW_EPSILON_%j.out\n#SBATCH --account=\n\nBGW=/path/to/where/you/built/BerkeleyGW/bin\nWFN_folder=/path/to/folder/that/contains/WFN/and/WFNq\n\nmkdir BGW_EPSILON_$SLURM_JOBID\nlfs setstripe -c 60 BGW_EPSILON_$SLURM_JOBID\ncd    BGW_EPSILON_$SLURM_JOBID\nln -s $BGW/epsilon.cplx.x .\nln -s  ../epsilon.inp .\nln -sfn  ${WFN_folder}/WFNq.h5      .   \nln -sfn  ${WFN_folder}/WFN.h5   ./WFN.h5\n\nulimit -s unlimited\nexport OMP_PROC_BIND=true\nexport OMP_PLACES=threads\nexport BGW_WFN_HDF5_INDEPENDENT=1\n\nexport OMP_NUM_THREADS=16\nsrun -n 32 -c 32 --cpu-bind=cores epsilon.cplx.x\n</code></pre> <p>This script will create a directory \"BGW_EPSILON_$SLURM_JOBID\" (where <code>$SLURM_JOBID</code> will be a numeric ID), stripe the directory with a stripe count of 60, link the epsilon executable, WFNq, and WFN files to the directory, and run BerkeleyGW with 32 GPUs.</p>"},{"location":"Documentation/Applications/berkeleygw_example/#building-instructions","title":"Building Instructions","text":"<p>First, download BerkeleyGW.</p> <p>Then, follow the build instructions in the \"building\" drop-downs below for the cluster you will be running on.</p> Building on Kestrel <p>The following <code>arch.mk</code> file was used to build BerkeleyGW-3.0 on Kestrel on (date).     Copy this arch.mk file into your BerkeleyGW directory.</p> <pre><code>COMPFLAG  = -DGNU\nPARAFLAG  = -DMPI -DOMP\nMATHFLAG  = -DUSESCALAPACK -DUNPACKED -DUSEFFTW3 -DHDF5\n\nFCPP    = /usr/bin/cpp -C\nF90free = mpifort -ffree-form -ffree-line-length-none -fopenmp -fno-second-underscore -cpp\nLINK    = mpifort -fopenmp\n# FHJ: -funsafe-math-optimizations breaks Haydock and doesn't give any significant speedup\nFOPTS   = -O3 -funroll-loops \nFNOOPTS = $(FOPTS)\nMOD_OPT = -J  \nINCFLAG = -I\n\nC_PARAFLAG  = -DPARA\nCC_COMP = mpiCC\nC_COMP  = mpicc\nC_LINK  = mpicc\nC_OPTS  = -O3 -ffast-math\nC_DEBUGFLAG = \n\nREMOVE  = /bin/rm -f\n\n# Math Libraries                                                                                                                                                                                            \nFFTWPATH     =  /projects/scatter/mylibraries_CentOS77/\n#/nopt/nrel/apps/fftw/3.3.3-impi-intel/\n#FFTWLIB      = $(FFTWPATH)/lib/libfftw3.a\nFFTWLIB      =  $(FFTWPATH)/lib/libfftw3_omp.a $(FFTWPATH)/lib/libfftw3.a\nFFTWINCLUDE  =  $(FFTWPATH)/include\n\nLAPACKLIB = /projects/scatter/mylibraries_CentOS77/lib/libopenblas.a\n\nSCALAPACKLIB = /projects/scatter/mylibraries_CentOS77/lib/libscalapack.a\n\nHDF5PATH      = /nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hdf5-1.10.6-dj4jq2ffttkdxksimqe47245ryklau4a\nHDF5LIB      =  ${HDF5PATH}/lib/libhdf5hl_fortran.a \\\n                ${HDF5PATH}/lib/libhdf5_hl.a \\\n                ${HDF5PATH}/lib/libhdf5_fortran.a \\\n                ${HDF5PATH}/lib/libhdf5.a /home/ohull/.conda-envs/bgw/lib/libsz.a -lz -ldl\nHDF5INCLUDE  = ${HDF5PATH}/include\n\nPERFORMANCE  =\n\nTESTSCRIPT = \n</code></pre> <p>Then, load the following modules:</p> <pre><code>module load gcc/8.4.0\nmodule load openmpi/3.1.6/gcc-8.4.0\nmodule load hdf5/1.10.6/gcc-ompi\n</code></pre> <p>Choose whether to use the real or complex flavor of BerkeleyGW by copying the corresponding file to flavor.mk. For example, for the complex version:</p> <p><code>cp flavor_cplx.mk flavor.mk</code></p> <p>Note</p> <p>If you are unsure which to use, select the complex flavor.</p> <p>Finally, compile the code. To view the available make targets, type <code>make</code>. To compile all BerkeleyGW executables, type: <pre><code>make cleanall\nmake all\n</code></pre></p> Building on Swift <p>TODO: add Swift build instructions</p>"},{"location":"Documentation/Applications/berkeleygw_example/#troubleshooting","title":"Troubleshooting","text":"<p>Include known problems and workarounds here, if applicable</p>"},{"location":"Documentation/Applications/berkeleygw_module/","title":"BerkeleyGW","text":"<p>Documentation: BerkeleyGW</p> <p>BerkeleyGW is a massively parallel many-body perturbation theory code capable of performing RPA, GW, and GW-BSE calculations, which can be used to investigate properties of materials with high accuracy.</p>"},{"location":"Documentation/Applications/berkeleygw_module/#getting-started","title":"Getting Started","text":"<p>This section provides the minimum amount of information needed to run a BerkeleyGW job on an NREL cluster.</p> <p>First, see which versions of BerkeleyGW are available with <code>module avail</code> and load your preferred version with <code>module load</code>:</p> <p><pre><code>module avail berkeleygw\n   berkeleygw/3.0.1-cpu    berkeleygw/3.0.1-gpu (D)\n</code></pre> The <code>module avail berkeleygw</code> command shows that two BerkeleyGW modules are available. To select the GPU-enabled version of BerkeleyGW, for example, we use the <code>module load</code> command:</p> <pre><code>module load berkeleygw/3.0.1-gpu\n</code></pre> <p>Next, create a job script. Below are example job scripts for the available NREL systems. Continuing the above example, we would select the \"Kestrel GPU\" example script.</p>"},{"location":"Documentation/Applications/berkeleygw_module/#sample-job-scripts","title":"Sample Job Scripts","text":"Kestrel CPU <pre><code>#!/bin/bash\n\n# This job requests 72 MPI tasks across 2 nodes (36 tasks/node) and no threading\n\n#SBATCH --time=01:00:00\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=36\n#SBATCH --partition=standard\n#SBATCH --account=\n\n\nsrun epsilon.cplx.x\n</code></pre> Kestrel GPU <p>Populate with Kestrel GPU submission script <pre><code>Put the script inside a code block\n</code></pre></p> Vermillion <p>Populate with Vermillion submission script <pre><code>Put the script inside a code block\n</code></pre></p> Swift <p>Populate with Swift submission script <pre><code>Put the script inside a code block\n</code></pre></p> <p>In the above scripts, be sure to include your allocation on the <code>#SBATCH --account</code> line.</p> <p>Save the submit script with the filename of your choice, e.g. \"bgw.in\"</p> <p>To submit the job to the scheduler, type:</p> <p><code>sbatch bgw.in</code></p>"},{"location":"Documentation/Applications/berkeleygw_module/#advanced","title":"Advanced","text":""},{"location":"Documentation/Applications/berkeleygw_module/#io-settings","title":"IO Settings","text":"<p>For large systems, the wavefunction binary file format yields significantly slower read-in times relative to an HDF5-format wavefunction file. The BerkeleyGW code includes utilities to convert wavefunction binary files to HDF5 format and vice-versa called hdf2wfn.x and wfn2hdf.x (see documentation). It is recommended to use HDF5-formatted wavefunction files where possible.</p>"},{"location":"Documentation/Applications/berkeleygw_module/#lustre-file-striping","title":"Lustre File Striping","text":"<p>BerkeleyGW supports wavefunction files in HDF5 format and binary format. Wavefunction inputs to BerkeleyGW can become large depending on the system under investigation. Large (TODO: define large for Kestrel. Probably &gt; 10 GB) HDF5 wavefunction files benefit from Lustre file striping, and the BerkeleyGW code can see major runtime speed-ups when using this feature.</p> <p>Tip</p> <p>Binary format wavefunction files do not benefit from Lustre file striping</p> <p>For more on Lustre file striping, see (TODO: documentation section on Lustre file striping?)</p>"},{"location":"Documentation/Applications/berkeleygw_module/#advanced-submission-script-example","title":"Advanced submission script example","text":"<p>Because multiple executables in BerkeleyGW require the WFN input files (WFN and WFNq), we can streamline the file linking inside a submission script. We can also include the Lustre file striping step in our submission script. The below example script shows how this can be done for the BerkeleyGW epsilon executable.</p> Advanced submit script <pre><code>#!/bin/bash\n#SBATCH -t 00:20:00\n#SBATCH -N 8\n#SBATCH --gpus-per-node=4\n#SBATCH -C gpu\n#SBATCH -o BGW_EPSILON_%j.out\n#SBATCH --account=\n\nWFN_folder=/path/to/folder/that/contains/WFN/and/WFNq\n\nmkdir BGW_EPSILON_$SLURM_JOBID\nlfs setstripe -c 60 BGW_EPSILON_$SLURM_JOBID\ncd    BGW_EPSILON_$SLURM_JOBID\nln -s  ../epsilon.inp .\nln -sf  ${WFN_folder}/WFNq.h5      .   \nln -sf  ${WFN_folder}/WFN.h5   ./WFN.h5\n\nulimit -s unlimited\nexport OMP_PROC_BIND=true\nexport OMP_PLACES=threads\nexport BGW_WFN_HDF5_INDEPENDENT=1\n\nexport OMP_NUM_THREADS=16\nsrun -n 32 -c 32 --cpu-bind=cores ./epsilon.cplx.x\n</code></pre> <p>Submit the job with <code>sbatch submit_script_filename</code></p>"},{"location":"Documentation/Applications/berkeleygw_module/#troubleshooting","title":"Troubleshooting","text":"<p>Include known problems and workarounds here, if applicable</p> <p>Maybe include here notable differences between NREL clusters for BerkeleyGW?</p>"},{"location":"Documentation/Applications/vasp/","title":"VASP: Vienna Ab initio Simulation Package","text":"<p>Documentation: VASP</p> <p>VASP computes an approximate solution to the many-body Schr\u00f6dinger equation, either within density functional theory (DFT), solving the Kohn-Sham equations, or within the Hartree-Fock (HF) approximation, solving the Roothaan equations. Hybrid functionals that mix the Hartree-Fock approach with density functional theory are implemented as well. Furthermore, Green's functions methods (GW quasiparticles, and ACFDT-RPA) and many-body perturbation theory (2nd-order M\u00f8ller-Plesset) are available in VASP.</p> <p>In VASP, central quantities, like the one-electron orbitals, the electronic charge density, and the local potential are expressed in plane wave basis sets. The interactions between the electrons and ions are described using norm-conserving or ultrasoft pseudopotentials, or the projector-augmented-wave method.</p> <p>To determine the electronic ground state, VASP makes use of efficient iterative matrix diagonalization techniques, like the residual minimization method with direct inversion of the iterative subspace (RMM-DIIS) or blocked Davidson algorithms. These are coupled to highly efficient Broyden and Pulay density mixing schemes to speed up the self-consistency cycle. </p>"},{"location":"Documentation/Applications/vasp/#getting-started","title":"Getting Started","text":""},{"location":"Documentation/Applications/vasp/#accessing-vasp","title":"Accessing VASP","text":"<p>The VASP license requires users to be a member of a \"workgroup\" defined by the University of Vienna or Materials Design. If you are receiving \"Permission denied\" errors when trying to use VASP, you must be made part of the \"vasp\" Linux group first. To join, please contact us with the following information: </p> <ul> <li>Your name</li> <li>The workgroup PI</li> <li>Whether you are licensed through Vienna (academic) or Materials Design, Inc. (commercial)</li> <li>If licensed through Vienna:<ul> <li>the e-mail address under which you are registered with Vienna as a workgroup member (this may not be the e-mail address you used to get an HPC account)</li> <li>Your VASP license ID</li> </ul> </li> <li>If licensed through Materials Design,<ul> <li>Proof of current licensed status</li> </ul> </li> </ul> <p>Once status can be confirmed, we can provide access to our VASP builds.</p>"},{"location":"Documentation/Applications/vasp/#setting-up-software-environment","title":"Setting Up Software Environment","text":"<p>First, see which modules of VASP are available:</p> <p><code>module avail vasp</code></p> <p>and select a version:</p> <p><code>module load vasp/6.1.2</code></p> <p>Three distinct executables have been made available:</p> <ol> <li><code>vasp_gam</code> is for Gamma-point-only runs typical for large unit cells</li> <li><code>vasp_std</code> is for general k-point meshes with collinear spins</li> <li><code>vasp_ncl</code> is for general k-point meshes with non-collinear spins</li> </ol> <p><code>vasp_gam</code> and <code>vasp_std</code> builds include the alternative optimization and transition state theory tools from University of Texas-Austin developed by Graeme Henkelman's group, and implicit solvation models from the University of Florida developed by Mathew and Hennig. </p>"},{"location":"Documentation/Applications/vasp/#example-job-scripts","title":"Example Job Scripts","text":"Kestrel CPU <pre><code>#!/bin/bash --login\n#SBATCH --job-name=job_name   # Replace job_name with your own\n#SBATCH --nodes=2             # This will deliver 2 36-core nodes\n#SBATCH --time=02:00:00       # REPLACE\n#SBATCH \u2013-account=account_id  # Replace with your allocation ID\n#SBATCH --error=%x-%A.err     # This will create stderr file &lt;job_name&gt;-&lt;jobid&gt;.err\n#SBATCH --output=%x-%A.out .  # This will create stdout file &lt;job_name&gt;-&lt;jobid&gt;.out\n\nmodule purge\nmodule load vasp/6.1.2\n\nJOB_BASENAME=$SLURM_JOB_NAME  # Captures the --job_name value from above\nSCRATCH=/scratch/$USER/$JOB_BASENAME\n\nif [ -d $SCRATCH ]\nthen\n   rm -rf $SCRATCH\nfi\nmkdir $SCRATCH\n\ncd $SLURM_SUBMIT_DIR  # Assumes you submitted job from directory containing VASP input\n\n# Minimal files needed for VASP run\ncp INCAR KPOINTS POSCAR POTCAR  $SCRATCH/.\n\ncd $SCRATCH\n\n# make sure to choose the correct VASP executable. Here we use vasp_gam\nsrun -n $SLURM_NTASKS vasp_gam &gt; $JOB_BASENAME.log\n</code></pre> Vermillion <p>If the submit script for Vermillion differs from Kestrel, then include a Vermillion example script here. If the submit script does not differ, then remove this section (starting from the <code>??? example \"Vermillion\"</code> line)</p> Swift <p>If the submit script for Swift differs from Kestrel, then include a Swift  example script here. If the submit script does not differ, then remove this section (starting from the <code>??? example \"Swift\"</code> line)</p> <p>Copy the job script to your working directory (where your INCAR, POSCAR, etc. are located) and submit the job with</p> <p><code>sbatch job_script_name</code></p>"},{"location":"Documentation/Applications/vasp/#supported-versions","title":"Supported Versions","text":"Kestrel Swift Vermillion 0.0.0 0.0.0 0.0.0"},{"location":"Documentation/Applications/vasp/#advanced","title":"Advanced","text":"<p>Include advanced user information about the code here </p> <p>One common \"advanced case\" might be that users want to build their own version of the code.</p>"},{"location":"Documentation/Applications/vasp/#building-from-source","title":"Building From Source","text":"<p>Here, give detailed and step-by-step instructions on how to build the code, if this step is necessary. Include detailed instructions for how to do it on each applicable HPC system. Be explicit in your instructions. Ideally a user reading one of the build sections can follow along step-by-step and have a functioning build by the end.</p> <p>If building from source is not something anyone would reasonably want to do, remove this section.</p> <p>Be sure to include where the user can download the source code</p> Build Instructions on Kestrel <p>Include here, for example, a Kestrel-specific makefile (see berkeleygw example page). This template assumes that we build the code with only one toolchain, which may not be the case. If someone might reasonably want to build with multiple toolchains, use the \"Multiple toolchain instructions on Kestrel\" template instead.</p> <p><pre><code>Include relevant commands in blocks.\n</code></pre> or as in-line <code>blocks</code></p> <p>Be sure to state how to set-up the necessary environment, e.g.:</p> <pre><code>module load gcc/8.4.0\nmodule load openmpi/3.1.6/gcc-8.4.0\nmodule load hdf5/1.10.6/gcc-ompi\n</code></pre> <p>Give instructions on compile commands. E.g., to view the available make targets, type <code>make</code>. To compile all program executables, type:</p> <pre><code>make cleanall\nmake all\n</code></pre> Building on Vermillion <p>information on how to build on Vermillion</p> Building on Swift <p>information on how to build on Swift</p>"},{"location":"Documentation/Applications/vasp/#troubleshooting","title":"Troubleshooting","text":"<p>Include known problems and workarounds here, if applicable</p>"},{"location":"Documentation/Debugging/","title":"Debugging Overview","text":"<p>Debugging code can be difficult on a good day, and parallel code introduces additional complications. Thankfully, we have a few tools available on NREL HPC machines to help us debug parallel programs. The intent of this guide is to serve as an overview of the types of information one can obtain with a parallel debugger on a supercomputer, and how this information can be used to solve problems in your code. Ultimately, the various parallel debugging and profiling tools tend to work similarly, providing information about individual task and thread performance, parallel memory usage and stack tracing, task-specific bottlenecks and fail points, and more.</p> <p>We offer several suites of parallel tools: </p> <ul> <li>the ARM Forge suite (DDT - debugger)</li> <li>the Intel oneAPI HPC Toolkit </li> <li>HPE\u2019s tool suite (Kestrel-only):<ul> <li>Cray Debugger Support Tools (CDST), including gdb4hpc, a command-line parallel debugger based on GDB.</li> <li>Cray Performance Measurement and Analysis Tools (CPMAT)</li> </ul> </li> </ul> <p>For a low-overhead serial debugger available on all NREL machines, see our GDB documentation.</p> <p>To skip to a walk-through example of parallel debugging, click here.</p>"},{"location":"Documentation/Debugging/#key-parallel-debugging-features","title":"Key Parallel Debugging Features","text":"<p>Parallel debuggers typically come equipped with the same features available on serial debuggers (breakpoint setting, variable inspection, etc.). However, unlike serial debuggers, parallel debuggers provide valuable MPI task- and thread-specific information, too. We present some key parallel features here.</p> <p>Note that while we present features of ARM DDT below, we stress that the many parallel debugging tools function in similar ways and offer similar features.</p>"},{"location":"Documentation/Debugging/#fail-points","title":"Fail points","text":"<p>Sometimes, some MPI tasks will fail at a particular point while others will not. This could be for a number of reasons (MPI task-defined variable goes out of bounds, etc.). Parallel debuggers can help us track down which tasks and/or threads are failing, and why. For example, in DDT, when the debugger detects that a task is failing, it prompts us to pause the program. On pausing, we can inspect which tasks are failing, and at which line in the code they are failing.</p>"},{"location":"Documentation/Debugging/#parallel-variable-inspection","title":"Parallel variable inspection","text":"<p>Other times, your code may not fail, but will produce an obviously incorrect answer. Such a situation is even less desirable than your code failing outright, since tracking down the problem variables and problem tasks is often more difficult.</p> <p>In these situations, the parallel variable inspection capabilities of parallel debuggers are valuable. We can first check if our code runs as expected when we run in serial. If so, the fault doesn\u2019t lie with the parallelism of your code, and you can proceed using serial debugging techniques. If the code succeeds in serial but yields incorrect results in parallel, then the code is likely afflicted with a parallel bug.</p> <p>Inspecting key parallel variables may help in identification of this bug. For example, we can inspect the variables that dictate how the parallel code is divided amongst MPI tasks. Is it as expected? Such a process will vary greatly on a code-by-code basis, but inspecting task-specific variables is a good place to start.</p> <p></p> <p>The above images shows a comparison across 8 MPI tasks of the first entry of the \u201cdoiownc\u201d variable of the BerkeleyGW code. In BerkeleyGW, this variable states whether or not the given MPI task \u201cowns\u201d a given piece of data. Here, we can see that Task 0 owns this piece of data, while tasks 1-7 do not.</p>"},{"location":"Documentation/Debugging/#parallel-memory-debugging","title":"Parallel memory debugging","text":"<p>In addition to detecting seg faults and out-of-bounds errors as discussed in fail points, parallel debuggers may offer more advanced memory debugging features. For example, DDT allows for advanced heap debugging. </p>"},{"location":"Documentation/Debugging/#walk-through","title":"Walk-through","text":"<p>To highlight some of the above features, we've introduced an out-of-bounds error to the BerkeleyGW code. We\u2019ve changed a writing of the <code>pol%gme</code> variable from:</p> <pre><code>do ic_loc = 1, peinf%ncownactual\n  do ig = 1, pol%nmtx\n    pol%gme(ig, ic_loc, iv_loc, ispin, irk, freq_idx) = ZERO\n  enddo\nenddo\n</code></pre> <p>To <pre><code>do ic_loc = 1, peinf%ncownactual\n  do ig = 1, pol%nmtx\n    pol%gme(ig, ic_loc, iv_loc, ispin, irk, freq_idx+1) = ZERO\n  enddo\nenddo\n</code></pre></p> <p>With this change, the sixth dimension of the array will go out of bounds. The details of the code aren\u2019t important, just the fact that we know the code will fail!</p> <p>Now, when we run the code in DDT, we receive the following error:</p> <p>[FAILURE ERROR WINDOW IMAGE]</p> <p>When we click <code>pause</code>, we are immediately taken to the line that caused the failure:</p> <p>[IMAGE OF pol%gme HIGHLIGHTED]</p> <p>We can inspect the local variables in the righthand-side box.</p> <p>One particularly useful feature is the \u201ccurrent stack.\u201d When we click this header, we are taken to a stack trace. When we click on each line in the stack trace, it takes us to the corresponding line in the corresponding source file, making stack tracing an error fast and simple. This is a common component of debuggers, even serial debuggers, but paired with our ability to choose which MPI task to focus on, this is a powerful feature! We get a task-specific view of the issue.</p> <p>Screenshot of GUI after pausing:</p> <ul> <li>Acknowledge that it will be more complicated in reality</li> <li>Main point is that it will point you to the exact line of code where it died, and also show you the whole stack trace/clickable</li> </ul>"},{"location":"Documentation/Debugging/#resources","title":"Resources","text":"<p>Parallel debugging tools are complex, and usage of these tools can sometimes end in attempts to \u201cdebug the debugger.\u201d Contact HPC Help if you need assistance in getting started with a parallel debugger on an NREL system.</p> <p>Refer to specific debugger pages, like the DDT page, on how to set-up and run the debugging program.</p>"},{"location":"Documentation/Debugging/gdb/","title":"GDB (GNU Debugger)","text":"<p>Documentation: GDB</p> <p>GDB is GNU's debugging tool. </p>"},{"location":"Documentation/Debugging/gdb/#getting-started","title":"Getting started","text":"<p>GDB is available on NREL machines and supports a number of languages, including C, C++, and Fortran. </p> <p>To use GDB, first load it into your environment:</p> <p><code>module load gdb</code></p> <p>Second, make sure the program you are attempting to debug has been compiled with the <code>-g</code> debug flag and with the <code>-O0</code> optimization flag to achieve the best results with gdb.</p> <p>For links to in-depth tutorials and walkthroughs of GDB features, please see Resources.</p>"},{"location":"Documentation/Debugging/gdb/#availability","title":"Availability","text":"Kestrel Swift Vermillion"},{"location":"Documentation/Debugging/gdb/#resources","title":"Resources","text":"<p>Gentle introduction to GDB: here</p> <p>Sample GDB session: here</p> <p>\"Print statement\"-style debugging with GDB: here</p>"},{"location":"Documentation/Libraries/howto/","title":"Libraries How-To: Linking Scientific Libraries","text":"<p>debugging index test here</p> <p>debugging index sublink test here</p> <p>fortran link test here</p> <p>fortran sublink test here</p> <p>This page is a tutorial explaining how to include scientific libraries when compiling software. </p> <p>There are a few common scientific libraries: LAPACK, BLAS, BLACS, scaLAPACK, fftw, hdf5, and others. These libraries are generally highly optimized, and many scientific programs favor use of these libraries over in-house implementations of similar functionality.  </p> <p>Scientific libraries can be packaged together, like in the Intel Math Kernel Library (MKL), or Cray\u2019s LibSci. They can also be built completely separately and act as standalone libraries. These libraries can be built with different MPI implementations and compiler choices. </p> <p>If you\u2019re building a code that relies on one or more of these libraries, you can choose how to include these libraries. By the end of this tutorial, how to include these libraries should be clearer. If you need help building a particular package on an NREL machine, please contact HPC help. </p>"},{"location":"Documentation/Libraries/howto/#makefiles-autoconf-and-cmake","title":"Makefiles, autoconf, and cmake","text":"<p>Build tools like make, autoconf, and cmake are convenient ways to automate the compilation of a code. If you\u2019re building a package, you may need to modify/customize how the code compiles, e.g., so it finds and includes the libraries you want. This may involve directly modifying the makefile, modifying the make.include (or make.inc, makefile.include, etc.) file, or using tools like autoconf or CMake to configure the makefile. </p> <p>Modifying a makefile (or make.include, etc.) so it compiles using the scientific libraries you want can be a daunting process. We\u2019ll go through a prototypical example and show how different libraries can be included in the build of a program. To do this, we\u2019ll use a makefile.include file for the electronic structure program VASP.</p> <p>Note</p> <p>We provide a walkthrough of linking scientific libraries using the VASP code as an example. This walkthrough tries to demonstrate key features of the general process of including scientific libraries in a build. We note that the exact build and modification process will vary between codes. Consulting the documentation of the code you\u2019re trying to build is always the best place to start. </p>"},{"location":"Documentation/Libraries/howto/#walkthrough","title":"Walkthrough","text":""},{"location":"Documentation/Libraries/howto/#overview","title":"Overview","text":"<p>We\u2019ll use the VASP makefile.include file as our walkthrough example. We can find a number of VASP makefile.include files here. We\u2019ll be looking specifically at this file.</p> <p>We\u2019ll take a look at building with Intel MKL and the HDF5 package. </p>"},{"location":"Documentation/Libraries/howto/#building-with-mkl-and-hdf5","title":"Building with MKL and HDF5","text":"<p>We want to build with MKL and HDF5. If we look at the VASP documentation, we see that LAPACK, scaLAPACK, BLAS, and FFTW are required. MKL covers all of these needs. Thus, we need to tell the makefile where to look for MKL.</p>"},{"location":"Documentation/Libraries/howto/#environment-preparation","title":"Environment Preparation","text":"<p>We need our MKL to be built with the same compilers and MPI implementation as we\u2019re building VASP with. Let\u2019s see what sorts of MKL builds are available to us. Using the following command to show what builds of mkl are available as a module: </p> <p><code>module avail 2&gt;&amp;1 | grep mkl</code> </p> <p>Yields the output: </p> <p><code>intel-oneapi-mkl/2023.0.0-intel      ucx/1.13.0</code> </p> <p>Thus, if we want to use the toolchains managed by NREL, we must use the Intel oneapi toolchain in our VASP build, since <code>intel-oneapi-mkl/2023.0.0-intel</code> is the only available mkl module. If you want to use a different toolchain, you could build MKL yourself, but that\u2019s outside the scope of this article. </p> <p>To \u201cuse the Intel oneapi toolchain\u201d means to use Intel compilers and Intel\u2019s implementation of MPI to compile VASP. We\u2019re doing this because mkl was built with this toolchain, and we want our toolchains to match as best as possible to minimize build errors and bugs. </p> <p>Let\u2019s prepare our environment to use this toolchain. First, </p> <p><code>module purge</code> </p> <p>To clear your environment. Now, we want the Intel oneapi mkl module, the Intel fortran compiler (ifort), and the Intel MPI fortran compiler (mpiifort). Type: </p> <p><code>module avail 2&gt;&amp;1 | grep oneapi</code> </p> <p>to see which modules are related to the intel-oneapi toolchain. We can locate the three we want: </p> <pre><code>module load intel-oneapi-mkl/2023.0.0-intel \nmodule load intel-oneapi-mpi/2021.8.0-intel \nmodule load intel-oneapi/2022.1.0 \n</code></pre> <p>How do we know these are the ones we want? The first line loads the mkl module. The second line gives us mpiifort, the Intel MPI fortran compiler, and the third line gives us ifort, the Intel Fortran compiler. (test the latter two with <code>which mpiifort</code> and <code>which ifort</code> -- you\u2019ll see that they\u2019re now in your path. If you <code>module purge</code> and try <code>which mpiifort</code> again, you\u2019ll see you\u2019re not able to find mpiifort anymore.) </p>"},{"location":"Documentation/Libraries/howto/#modifying-the-makefile-for-mkl","title":"Modifying the Makefile for MKL","text":"<p>Now that we have the toolchain loaded into our environment, let\u2019s take a look at the actual makefile.include file (link to file here). There are two important sections for the purpose of getting the code to build. The first: </p> <pre><code>CPP         = fpp -f_com=no -free -w0  $*$(FUFFIX) $*$(SUFFIX) $(CPP_OPTIONS) \nFC          = mpiifort -qopenmp \nFCL         = mpiifort \n</code></pre> <p>The first line says that the compiler pre-processor will be fpp (try <code>which fpp</code> and you should get an output <code>/sfs/nopt/nrel/apps/compilers/01-23/spack/opt/spack/linux-rhel8-icelake/gcc-8.4.0/intel-oneapi-compilers-2022.1.0-wosfexnwo5ag3gyfoco2w6upcew5yj6f/compiler/2022.1.0/linux/bin/intel64/fpp</code>, confirming that we\u2019re pulling fpp from intel-oneapi).  </p> <p>The second and third lines say that we\u2019ll be using Intel\u2019s MPI (Try <code>which mpiifort</code> to confirm that it is in your path). FC is the \u201cFortran Compiler\u201d and FCL is the corresponding linker. Line 14 additionally says we\u2019ll be compiling with openmp. Different compilers have different executable names (e.g. mpiifort for Intel MPI fortran compiler, mpifort for GNU). See the Fortran documentation page for a complete list. </p> <p>The next important section is given below: </p> <pre><code># Intel MKL (FFTW, BLAS, LAPACK, and scaLAPACK) \n# (Note: for Intel Parallel Studio's MKL use -mkl instead of -qmkl) \nFCL        += -qmkl \nMKLROOT    ?= /path/to/your/mkl/installation \nLLIBS      += -L$(MKLROOT)/lib/intel64 -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64\nINCS        =-I$(MKLROOT)/include/fftw \n</code></pre> <p>This makefile.include file has been provided to us by VASP. Our job here is two-fold:</p> <ol> <li>To ensure that we tell make (via the makefile.include file) the correct place to find MKL, I.e., to ensure that <code>MKLROOT</code> in the makefile.include file is set correctly.</li> <li>To ensure that we tell make the correct libraries to reference within <code>MKLROOT</code>.</li> </ol> <p>To do step 1, first type:</p> <p><code>module list</code> </p> <p>To see the modules you\u2019ve loaded into your environment. You should have <code>intel-oneapi-mkl/2023.0.0-intel</code> in the list.  If not, review the environment preparation section. Now, we use the <code>module show</code> command to find the root directory of mkl: </p> <p><code>module show intel-oneapi-mkl/2023.0.0-intel</code> </p> <p>We see in the output of this command the following line: </p> <p><code>setenv      MKLROOT /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mkl-2023.0.0-gnkrgwyxskxitvptyoubqaxlhh2v2re2/mkl/2023.0.0</code> </p> <p>If we type <code>echo $MKLROOT</code>, we can confirm that this environment variable is properly set from when we ran the command <code>module load intel-oneapi-mkl/2023.0.0-intel</code>. In the VASP makefile, we have <code>MKLROOT    ?= /path/to/your/mkl/installation</code>. The ?= means that this variable will not be set if <code>MKLROOT</code> has already been set. So, we can ignore this line if we\u2019d like. However, to be safe, we should simply copy the path of the MKL root directory to this line in makefile.include, so that this line now reads: </p> <p><code>MKLROOT    ?= /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mkl-2023.0.0-gnkrgwyxskxitvptyoubqaxlhh2v2re2/mkl/2023.0.0</code> </p> <p>Tip</p> <p>The name of the environment variable for mkl\u2019s root directory set by its module (<code>MKLROOT</code>, set when we <code>module load intel-oneapi-mkl/2023.0.0-intel</code>) is not necessarily going to match the corresponding root directory variable in a given makefile. It did in this instance, but that\u2019s not guaranteed. The VASP makefile.include could have just as easily used <code>MKL_ROOT</code>, instead of <code>MKLROOT</code>. This is one reason why it\u2019s safer to use <code>module show</code> to find the path of the root directory, then copy this path into the makefile, rather than rely on environment variables.  </p> <p>To do step 2, we should first look at the contents of <code>$MKLROOT</code>. To show the contents of the MKL directory, type</p> <p><code>ls /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mkl-2023.0.0-gnkrgwyxskxitvptyoubqaxlhh2v2re2/mkl/2023.0.0</code></p> <p>We should obtain the following output:</p> <p><code>benchmarks  bin  env  examples  include  interfaces  lib  licensing  modulefiles  tools</code></p> <p>If we look closely at the makefile, we see beneath the <code>MKLROOT</code> line the following: <pre><code>MKLROOT    ?= /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mkl-2023.0.0-gnkrgwyxskxitvptyoubqaxlhh2v2re2/mkl/2023.0.0\nLLIBS      += -L$(MKLROOT)/lib/intel64 -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64\n</code></pre></p> <p>the <code>LLIBS</code> line is telling make which libraries in particular to pick out. </p> <p>So, we want to go into the lib directory, and then the intel64 directory (since LLIBS is pointing to <code>$MKLROOT/lib/intel64</code>). Let's see what's inside with the <code>ls</code> command:</p> <p><code>ls  /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mkl-2023.0.0-gnkrgwyxskxitvptyoubqaxlhh2v2re2/mkl/2023.0.0/lib/intel64</code></p> <p>There's a lot of stuff in this directory! VASP helps us by telling us we need the <code>mkl_scalapack_lp64</code> and <code>mkl_blacs_openmpi_lp64</code> builds specifically. You won't always be told exactly which libraries, and figuring this out, if the information is not provided to you in the package documentation, can require some tinkering.</p> <p>In general, the <code>.a</code> extension is for static linking, and the <code>.so</code> extension is for dynamic linking. For MKL in particular, the part <code>ilp64</code> vs <code>lp64</code> refer to two different interfaces to the MKL library. </p> <p>Tip</p> <p>Notice that, inside <code>$MKLROOT/lib/intel64</code>, the  filenames all start with <code>libmkl</code>, but in our makefile, we reference <code>lmkl_scalapack_lp64</code>. That's not a file in <code>$MKLROOT/lib/intel64</code>, but <code>libmkl_scalapack_lp64.so</code> is. The notation is that \"big L\" references the directories that the libraries are in, and the \"little l\" references the particular libraries. For example: <pre> LLIBS += -L$(MKLROOT)/lib/intel64 </pre> <pre> -lmkl_scalapack_lp64</pre> This is just a convention, but is important to get right because your compile will fail otherwise.</p> <p>Now that we have the correct <code>MKLROOT</code> set in the makefile.include, and we have an idea about how it's referencing the libraries within, we can move on to linking the HDF5 library.</p>"},{"location":"Documentation/Libraries/howto/#modifying-the-makefile-for-hdf5","title":"Modifying the Makefile for HDF5","text":"<p>Because HDF5 is an optional library, we could compile the code now if we wanted to. However, for the sake of practice, let\u2019s uncomment the block in the makefile.include file related to HDF5 and repeat the exercise of linking a library: </p> <pre><code># HDF5-support (optional but strongly recommended) \nCPP_OPTIONS+= -DVASP_HDF5 \nHDF5_ROOT  ?= /path/to/your/hdf5/installation \nLLIBS      += -L$(HDF5_ROOT)/lib -lhdf5_fortran \nINCS       += -I$(HDF5_ROOT)/include \n</code></pre> <p>Our job, again, is to give the makefile the correct directions to our library. In this case, it\u2019s HDF5. Let\u2019s see which HDF5 modules are available: </p> <p><code>module avail hdf5</code> </p> <p>Returns </p> <p><code>hdf5/1.12.2-intel-oneapi-mpi-intel hdf5/1.12.2-openmpi-gcc</code> </p> <p>So, we see that HDF5 has been built with the intel-oneapi-mpi toolchain, and also with the GCC/openmpi toolchain. Since we\u2019re building vasp using the intel-oneapi toolchain, we need to load the corresponding module: </p> <p><code>module load hdf5/1.12.2-intel-oneapi-mpi-intel</code> </p> <p>Again, we must locate the root directory: </p> <p><code>module show hdf5/1.12.2-intel-oneapi-mpi-intel</code> </p> <p>We see the line for setting the HDF5 root directory environment variable: </p> <p><code>setenv      HDF5_ROOT_DIR /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/hdf5-1.12.2-dzgeixsm2cd3mupx4ti77ozeh7rh6zdo</code> </p> <p>Like before, we copy this path into our makefile.include: </p> <pre><code># HDF5-support (optional but strongly recommended) \nCPP_OPTIONS+= -DVASP_HDF5 \nHDF5_ROOT  ?= /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/hdf5-1.12.2-dzgeixsm2cd3mupx4ti77ozeh7rh6zdo \nLLIBS      += -L$(HDF5_ROOT)/lib -lhdf5_fortran \nINCS       += -I$(HDF5_ROOT)/include \n</code></pre> <p>We\u2019re ready to compile! In the case of VASP, the compile command is <code>make DEPS=1 std</code> but in general, the command may be <code>make all</code> or similar (consult the documentation of the code you\u2019re trying to build). </p> <p>If you\u2019re working with a code that has a testsuite, now is a good time to run the testsuite to make sure that your compile was successful. </p>"},{"location":"Documentation/Libraries/howto/#summary-of-steps","title":"Summary of Steps","text":"<ol> <li>Download the source code of the package you\u2019re trying to build. This will generally be found on the website of the package. </li> <li>Consult the documentation of the package to find out what scientific libraries are needed, and if the package developers provide guidance on what toolchains/libraries are best </li> <li>Determine the availability of the needed scientific libraries.  <ol> <li>Can a \u201clibrary-of-libraries\u201d like MKL or LibSci be used? </li> <li>Does NREL support the library as a module?  <ol> <li>If so, determine the toolchain it was built with (usually given in the name of the module). If the toolchain is not clear from the name of the module, try the <code>ldd</code> command (e.g., <code>ldd path/to/executable/executable</code>), which will show you the dynamically linked libraries of the executable.</li> </ol> </li> </ol> </li> <li>Prepare your environment <ol> <li><code>module load</code> the necessary modules to prepare your environment. (See  environment preparation step of VASP example) </li> </ol> </li> <li>Prepare your makefile <ol> <li>Make sure that the compilers and (optional) MPI used in the makefile match what is used to build your scientific libraries as best as possible </li> <li>Make sure that the paths to the scientific libraries in the makefile match the path given by the <code>module show</code> command </li> <li>Make sure the proper \u201clittle L\u201d libraries are referenced in the makefile </li> </ol> </li> <li>Compile!</li> </ol>"},{"location":"Documentation/Libraries/howto/#questions","title":"Questions?","text":"<p>If you\u2019re still stuck and unable to successfully link the scientific libraries you need, get in contact with HPC help.</p>"},{"location":"Documentation/Libraries/intro_libraries/","title":"Scientific Libraries Overview","text":"<p>Scientific math libraries are a collection of highly optimized software tools that provide functions and algorithms for performing mathematical operations commonly used in scientific applications. They provide developers with a variety of tools for solving complex problems. These libraries are highly optimized for performance and generally designed to be portable across different platforms and operating systems. </p> <p>We support some of the most widely used scientific math libraries including:</p> <ul> <li>MKL </li> <li>LibSci (Kestrel only)</li> <li>FFTW </li> <li>LAPACK</li> <li>scaLAPACK</li> <li>HDF5 </li> </ul> <p>For details on how to build an application with scientific libraries, see our how-to guide</p>"},{"location":"Documentation/Libraries/intro_libraries/#mkl","title":"MKL","text":"<p>Documentation: MKL</p> <p>MKL is a collection of highly optimized mathematical libraries provided by Intel for use in scientific, engineering, and financial applications. The library is designed to take full advantage of the latest Intel processors, including multi-core processors, and can significantly improve the performance of numerical applications. Core math functions include: </p> <ul> <li>BLAS (Basic Linear Algebra Subroutines) </li> <li>LAPACK (Linear Algebra routines) </li> <li>ScaLAPACK (parallel Linear Algebra routines) </li> <li>Sparse Solvers </li> <li>Fast Fourier Transforms </li> <li>Vector Math </li> </ul>"},{"location":"Documentation/Libraries/intro_libraries/#libsci","title":"LibSci","text":"<p>Documentation: LibSci</p> <p>LibSci is a collection of numerical libraries developed by AMD for scientific and engineering computing. LibSci is optimized for performance on AMD processors, including multi-core processors, and supports both single-precision and double-precision arithmetic. It also includes multithreading support for parallel execution on shared-memory systems. Like MKL, LibSci includes the following math functions: </p> <ul> <li>BLAS (Basic Linear Algebra Subroutines) </li> <li>CBLAS (C interface to the legacy BLAS) Note: not sure if this is also in MKL? </li> <li>BLACS (Basic Linear Algebra Communication Subprograms) </li> <li>LAPACK (Linear Algebra routines) </li> <li>ScaLAPACK (parallel Linear Algebra routines) </li> </ul> <p>And additionally, libraries that are unique to Cray systems including:  * IRT (Iterative Refinement Toolkit) - a library of solvers and tools that provides solutions to linear systems using single-precision factorizations while preserving accuracy through mixed-precision iterative refinement.  * CrayBLAS - a library of BLAS routines autotuned for Cray XC series systems through extensive optimization and runtime adaptation.  </p>"},{"location":"Documentation/Libraries/intro_libraries/#fftw","title":"FFTW","text":"<p>Documentation: FFTW</p> <p>FFTW is a C library for computing discrete Fourier transforms of arbitrary input sizes and dimensions. It is optimized for speed and can perform discrete Fourier transforms up to several orders of magnitude faster than other commonly available Fourier transform libraries. FFTW supports both single-precision and double-precision transforms, as well as multithreading for parallel execution on shared-memory systems.</p>"},{"location":"Documentation/Libraries/intro_libraries/#lapack-and-scalapack","title":"LAPACK and scaLAPACK","text":"<p>Documentation: LAPACK, scaLAPACK</p> <p>LAPACK is a highly optimized library of linear algebra routines written in Fortran 90. These routines include matrix multiplication, factorization (LU, Cholesky, QR, etc.) least squares solutions of linear systems, eigenvalue problems, and many others. LAPACK routines are available in both single and double precision, and for complex and real numbers.</p> <p>LAPACK depends on BLAS (Basic Linear Algebra Subprograms).</p> <p>ScaLAPACK is a parallel-distributed version of LAPACK (i.e., scalaPACK is MPI-parallel)</p> <p>Both LAPACK and ScaLAPACK are available as either standalone libraries (<code>netlib-lapack</code>), or as part of the \"package-of-packages\" libraries MKL and LibSci.</p>"},{"location":"Documentation/Libraries/intro_libraries/#hdf5","title":"HDF5","text":"<p>Documentation: HDF5</p> <p>HDF5 is a versatile data storage and management library designed for storing and exchanging large and complex data collections. It provides a powerful and flexible data model for representing and organizing data, as well as a variety of high-level programming interfaces for accessing and manipulating data. HDF5 supports a wide range of data types and can handle data sets of virtually unlimited size.</p>"},{"location":"Documentation/Libraries/intro_libraries/#additional-resources","title":"Additional Resources","text":"<p>For a detailed guide on how to include scientific libraries when compiling programs, see our guide.</p>"},{"location":"Documentation/ProgrammingLanguages/fortran/","title":"Fortran","text":""},{"location":"Documentation/ProgrammingLanguages/fortran/#getting-started","title":"Getting Started","text":"<p>This section provides the minimum amount of information necessary to successfully compile a basic Fortran code, and then a basic Fortran MPI code. See External Resources for general Fortran language tutorials and Fortran-MPI tutorials. See Compilers for detailed compiler and programming environment information.</p>"},{"location":"Documentation/ProgrammingLanguages/fortran/#hello-world","title":"Hello World","text":"<p>First, we must choose the compiler with which to compile our program. We can choose between the GNU, Intel, Nvidia, and Cray compilers. </p> <p>To see available versions of a chosen compiler, use <code>module avail</code>. For this example, we'll use gfortran, which is part of GNU's <code>gcc</code> package:</p> <pre><code>module avail gcc\n   gcc/10.3.0          gcc/11.2.0          gcc/12.1.0(default)\n</code></pre> <p>To load a particular compiler, use <code>module load</code>. We'll use gcc/12.1.0.</p> <pre><code>module load gcc/12.1.0\n</code></pre> <p>Create a file named hello.f90, and save the following text to the file:</p> <pre><code>PROGRAM hello\n\nwrite(*,*) \"Hello World\"\n\nEND PROGRAM hello\n</code></pre> <p>Now, we can compile the program with the following command:</p> <p><code>gfortran hello.f90 -o hello</code></p> <p>This creates an executable named <code>hello</code>. Execute it by typing the following into your terminal:</p> <p><code>./hello</code></p> <p>It should return the following output:</p> <p><code>Hello World</code></p>"},{"location":"Documentation/ProgrammingLanguages/fortran/#hello-world-in-mpi-parallel","title":"Hello World in MPI Parallel","text":"<p>Now that we have a working Hello World program, let's modify it to run on multiple MPI tasks.</p> <p>On Kestrel, there are multiple implementations of MPI available. We can choose between OpenMPI, Intel MPI, MPICH, and Cray MPICH. These MPI implementations are associated with an underlying Fortran compiler. For example, if we type:</p> <p><code>module avail openmpi</code></p> <p>we find that both <code>openmpi/4.1.4-gcc</code> and <code>openmpi/4.1.4-intel</code> are available.</p> <p>Let's choose the openmpi/gcc combination:</p> <p><code>module load openmpi/4.1.4-gcc</code></p> <p>Now, create a new file named <code>hello_mpi.f90</code> and save the following contents to the file:</p> <pre><code>PROGRAM hello_mpi\ninclude 'mpif.h'\n\ninteger :: ierr, my_rank, number_of_ranks\n\ncall MPI_INIT(ierr)\ncall MPI_COMM_SIZE(MPI_COMM_WORLD, number_of_ranks, ierr)\ncall MPI_COMM_RANK(MPI_COMM_WORLD, my_rank, ierr)\n\nwrite(*,*) \"Hello World from MPI task: \", my_rank, \"out of \", number_of_ranks\n\ncall MPI_FINALIZE(ierr)\n\nEND PROGRAM hello_mpi\n</code></pre> <p>To compile this program, type:</p> <p><code>mpif90 hello_mpi.f90 -o hello_mpi</code></p> <p>To run this code on the login node, type:</p> <p><code>mpirun -n 4 ./hello_mpi</code></p> <p>You should receive a similar output to the following (the rank ordering may differ):</p> <pre><code> Hello World from MPI task:            1 out of            4\n Hello World from MPI task:            2 out of            4\n Hello World from MPI task:            3 out of            4\n Hello World from MPI task:            0 out of            4\n</code></pre> <p>Generally, we don't want to run MPI programs on the login node! Let's submit this as a job to the scheduler. Create a file named <code>job.in</code> and modify the file to contain the following:</p> <p><pre><code>#!/bin/bash\n\n#SBATCH --time=00:01:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --partition=standard\n#SBATCH --account=&lt;your account here&gt;\n\nmodule load openmpi/4.1.4-gcc\n\nsrun -n 4 ./hello_mpi &amp;&gt; hello.out\n</code></pre> Be sure to replace the <code>&lt;your account here&gt;</code> with your account name.</p> <p>Submit the job:</p> <p><code>sbatch job.in</code></p> <p>When the job is done, the file hello.out should contain the same output as you found before (the ordering of ranks may differ).</p>"},{"location":"Documentation/ProgrammingLanguages/fortran/#compilers","title":"Compilers","text":""},{"location":"Documentation/ProgrammingLanguages/fortran/#standalone-compilers","title":"Standalone compilers:","text":"Compiler Compiler Executable Module Avail Systems available on GNU (gcc) gfortran gcc Kestrel(Eagle), Swift, Vermillion Intel ifort intel-oneapi Kestrel(Eagle), Swift, Vermillion Intel ifort intel-classic Kestrel"},{"location":"Documentation/ProgrammingLanguages/fortran/#fortran-mpi-compilers","title":"Fortran-MPI compilers","text":"Compiler MPI Compiler Executable Module Avail Systems available on GNU (gcc) openmpi mpifort openmpi Kestrel(Eagle), Swift, Vermillion intel openmpi mpifort openmpi/4.1.x-intel Kestrel(Eagle) intel intel mpiifort intel-oneapi-mpi Kestrel, Swift, Vermillion gcc MPICH mpifort mpich Kestrel, Swift, Vermillion intel MPICH mpifort mpich/4.0.2-intel Kestrel only cray MPICH ftn cray-mpich Kestrel only"},{"location":"Documentation/ProgrammingLanguages/fortran/#external-resources","title":"External Resources","text":"<ul> <li>Basic Fortran Tutorial </li> <li>Detailed Fortran Tutorial</li> <li>Fortran/MPI on an HPC Tutorial</li> </ul>"},{"location":"Documentation/ProgrammingLanguages/fortran_kestrel/","title":"Fortran","text":""},{"location":"Documentation/ProgrammingLanguages/fortran_kestrel/#getting-started","title":"Getting Started","text":"<p>This section provides the minimum amount of information necessary to successfully compile a basic Fortran code, and then a basic Fortran MPI code. See External Resources for general Fortran language tutorials and Fortran-MPI tutorials. See Compilers for detailed compiler and programming environment information.</p>"},{"location":"Documentation/ProgrammingLanguages/fortran_kestrel/#hello-world","title":"Hello World","text":"<p>First, we must choose the compiler with which to compile our program. We can choose between the GNU, Intel, Nvidia, and Cray compilers. </p> <p>To see available versions of a chosen compiler, use <code>module avail</code>. For this example, we'll use gfortran, which is part of GNU's <code>gcc</code> package:</p> <pre><code>module avail gcc\n   gcc/10.3.0          gcc/11.2.0          gcc/12.1.0(default)\n</code></pre> <p>To load a particular compiler, use <code>module load</code>. We'll use gcc/12.1.0.</p> <pre><code>module load gcc/12.1.0\n</code></pre> <p>Create a file named hello.f90, and save the following text to the file:</p> <pre><code>PROGRAM hello\n\nwrite(*,*) \"Hello World\"\n\nEND PROGRAM hello\n</code></pre> <p>Now, we can compile the program with the following command:</p> <p><code>gfortran hello.f90 -o hello</code></p> <p>This creates an executable named <code>hello</code>. Execute it by typing the following into your terminal:</p> <p><code>./hello</code></p> <p>It should return the following output:</p> <p><code>Hello World</code></p>"},{"location":"Documentation/ProgrammingLanguages/fortran_kestrel/#hello-world-in-mpi-parallel","title":"Hello World in MPI Parallel","text":"<p>Now that we have a working Hello World program, let's modify it to run on multiple MPI tasks.</p> <p>On Kestrel, there are multiple implementations of MPI available. We can choose between OpenMPI, Intel MPI, MPICH, and Cray MPICH. These MPI implementations are associated with an underlying Fortran compiler. For example, if we type:</p> <p><code>module avail openmpi</code></p> <p>we find that both <code>openmpi/4.1.4-gcc</code> and <code>openmpi/4.1.4-intel</code> are available.</p> <p>Let's choose the openmpi/gcc combination:</p> <p><code>module load openmpi/4.1.4-gcc</code></p> <p>Now, create a new file named <code>hello_mpi.f90</code> and save the following contents to the file:</p> <pre><code>PROGRAM hello_mpi\ninclude 'mpif.h'\n\ninteger :: ierr, my_rank, number_of_ranks\n\ncall MPI_INIT(ierr)\ncall MPI_COMM_SIZE(MPI_COMM_WORLD, number_of_ranks, ierr)\ncall MPI_COMM_RANK(MPI_COMM_WORLD, my_rank, ierr)\n\nwrite(*,*) \"Hello World from MPI task: \", my_rank, \"out of \", number_of_ranks\n\ncall MPI_FINALIZE(ierr)\n\nEND PROGRAM hello_mpi\n</code></pre> <p>To compile this program, type:</p> <p><code>mpif90 hello_mpi.f90 -o hello_mpi</code></p> <p>To run this code on the login node, type:</p> <p><code>mpirun -n 4 ./hello_mpi</code></p> <p>You should receive a similar output to the following (the rank ordering may differ):</p> <pre><code> Hello World from MPI task:            1 out of            4\n Hello World from MPI task:            2 out of            4\n Hello World from MPI task:            3 out of            4\n Hello World from MPI task:            0 out of            4\n</code></pre> <p>Generally, we don't want to run MPI programs on the login node! Let's submit this code as a job. Create a file named <code>job.in</code> and modify the file to contain the following:</p> <pre><code>#!/bin/bash\n\n#SBATCH --time=00:01:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --partition=debug\n#SBATCH --account=\n\nmodule load openmpi/4.1.4-gcc\n\nsrun -n 4 ./hello_mpi &amp;&gt; hello.out\n</code></pre> <p>Submit the job by typing:</p> <p><code>sbatch job.in</code></p> <p>When the job is done, the file hello.out should contain the same output as you found before (the ordering of ranks may differ).</p>"},{"location":"Documentation/ProgrammingLanguages/fortran_kestrel/#compilers","title":"Compilers","text":""},{"location":"Documentation/ProgrammingLanguages/fortran_kestrel/#standalone-compilers","title":"Standalone compilers:","text":"Compiler Compiler Executable Module Avail Available Versions GNU (gcc) gfortran gcc 12.1.0, 11.2.0, 10.3.0, 10.1.0 Intel ifort intel-oneapi 2023.0.0, 2022.1.0, 2021.8.0 Intel ifort intel-classic 2022.1.0 Cray ftn cce 14.0.4 Nvidia (nvhpc) nvfortran nvhpc 22.7"},{"location":"Documentation/ProgrammingLanguages/fortran_kestrel/#programming-environments","title":"Programming Environments:","text":"<p>Note that in addition to the standalone Fortran compilers listed above, the various available programming environments available on the HPC (use <code>module avail PrgEnv</code> to view available programming environments) contain an associated Fortran compiler. For example, <code>module load PrgEnv-gnu/8.3.3</code> will load gfortran/12.1.0 into your environment, which you can verify with the <code>gfortran --version</code> command. If using a PrgEnv from a given creator, the loaded fortran compiler executable will be given by the same name as listed in the above table.</p> Creator Compiler Executable Module Avail Fortran Compiler Version GNU gfortran PrgEnv-gnu/8.3.3 12.1.0 GNU gfortran PrgEnv-gnu-amd/8.3.3 broken? Intel ifort PrgEnv-intel/8.3.3 2021.6.0 Cray ftn PrgEnv-Cray/8.3.3 14.0.4 Nvidia nvfortran PrgEnv-nvhpc/8.3.3 22.7.0 Nvidia nvfortran PrgEnv-nvidia/8.3.3 broken?"},{"location":"Documentation/ProgrammingLanguages/fortran_kestrel/#fortran-mpi-compilers","title":"Fortran-MPI compilers","text":"Compiler MPI Compiler Executable Module Avail GNU (gcc) openmpi mpifort openmpi/4.1.4-gcc intel openmpi mpifort openmpi/4.1.4-intel intel intel mpiifort intel-oneapi-mpi/2021.8.0-intel gcc MPICH mpifort mpich/4.0.2-gcc intel MPICH mpifort mpich/4.0.2-intel cray MPICH ftn cray-mpich/8.1.20"},{"location":"Documentation/ProgrammingLanguages/fortran_kestrel/#troubleshooting","title":"Troubleshooting","text":"<p>Include known problems and workarounds here, if applicable</p>"},{"location":"Documentation/ProgrammingLanguages/fortran_kestrel/#external-resources","title":"External Resources","text":"<ul> <li>Basic Fortran Tutorial </li> <li>Fortran/MPI on an HPC Tutorial</li> </ul>"}]}